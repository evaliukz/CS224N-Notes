# Word2Vec 核心知识点总结（CS224N）

## 1. Word2Vec 是什么？

**一句话定义：**  
Word2Vec 是一种通过「预测上下文」的自监督任务，学习词的低维连续向量表示（word embeddings）的方法。

**核心目标：**
- 将离散的词 → 连续向量空间
- 语义相似的词 → 向量距离更近
- 语义关系 → 向量差具有意义

---

## 2. 理论基础：Distributional Hypothesis

> *“A word is characterized by the company it keeps.”*

含义：
- 词的意义来自它出现的上下文
- 出现在相似上下文中的词，语义相似

Word2Vec 的所有设计都围绕这一假设。

---

## 3. Word2Vec 的两种模型

### 3.1 CBOW（Continuous Bag of Words）

- **输入**：上下文词（求和 / 平均）
- **输出**：中心词
- **特点**：
  - 训练快
  - 对高频词效果好
  - 语义较粗

---

### 3.2 Skip-gram（CS224N & 工业界重点）

- **输入**：中心词
- **输出**：上下文词
- **特点**：
  - 对低频词效果好
  - 语义更精细
  - 训练更慢但质量更高

---

## 4. 训练数据如何生成？

使用滑动窗口（window size = m）：

给定序列：
w1, w2, ..., wt, ..., wT

Skip-gram 生成训练样本：

(wt, wt-m), ..., (wt, wt-1), (wt, wt+1), ..., (wt, wt+m)


这是一个**自监督学习过程**。

---

## 5. 输入向量 vs 输出向量（重点）

### 5.1 两套向量的定义

对每个词 w：
- **输入向量（input embedding）**：`v_w`
- **输出向量（output embedding）**：`u_w`

含义：
- `v_w`：词作为**中心词 / 条件词**时的表示
- `u_w`：词作为**上下文词 / 被预测目标**时的表示

---

### 5.2 为什么需要两套向量？

因为 Skip-gram 建模的是条件概率：

P(context | center)


这是一个**非对称问题**：
- 中心词 = input
- 上下文词 = output（类似分类器的 label）

---

### 5.3 训练后用哪一套？

常见做法：
- 只用 `v_w`（最常见）
- 或 `(v_w + u_w) / 2`

---

## 6. 模型结构（Skip-gram）

- 输入：one-hot(center word)
- 通过 embedding lookup 得到 `v_c`
- 与 `u_o` 做点积
- 通过 softmax 或 negative sampling 计算概率

本质是一个 **极浅的神经网络（无非线性）**。

---

## 7. 目标函数（Full Softmax）

对一个正样本对 `(c, o)`：

P(o | c) = exp(u_o · v_c) / Σ_w exp(u_w · v_c)

Loss：

L = -log P(o | c)

问题：
- 分母需要遍历整个词表
- 计算复杂度 O(|V|)

---

## 8. Negative Sampling（工程核心）

### 8.1 核心思想

把「多分类问题」转成「二分类问题」：

- 正样本：真实 (center, context)
- 负样本：随机采样的噪声词

---

### 8.2 Loss 函数

对一个正样本 `(c, o)` 和 K 个负样本 `n1..nK`：

L = -log σ(u_o · v_c)
- Σ_i log σ(-u_ni · v_c)


其中：
- σ 是 sigmoid 函数
- K 通常为 5~20

---

### 8.3 复杂度优势

- 每步只更新：
  - 中心词 `v_c`
  - 正样本 `u_o`
  - K 个负样本 `u_n`
- 复杂度：`O(K · d)`（可扩展）

---

## 9. 负采样分布（常考）

负样本采样概率：
P(w) ∝ (词频)^(3/4)

目的：
- 降低高频词（the, of）的影响
- 保留低频词
- 平衡训练信号

---

## 10. 训练技巧（Tricks）

### 10.1 Subsampling 高频词
- 高频词以更低概率参与训练
- 提升语义词质量 + 加速训练

### 10.2 Dynamic Window
- 窗口大小随机 ≤ m
- 更强调近邻词

---

## 11. 为什么会出现线性语义关系？

例如：
king - man + woman ≈ queen

原因：
- Skip-gram + Negative Sampling
- 隐式学习词的共现统计结构
- 某些语义特征在向量空间中呈现近似线性方向

---

## 12. Word2Vec 的局限性

- ❌ 一词一向量（无法处理多义词）
- ❌ 无法建模上下文语义
- ❌ 无法表示句子级语义

直接推动了：
- ELMo（上下文词向量）
- BERT / GPT（上下文 + 深层模型）

---

## 13. 与现代 LLM 的关系

- Word2Vec：
  - input embedding
  - output embedding（softmax weights）
- Transformer / LLM：
  - token embedding
  - output projection matrix
  - 常用 **weight tying**

Word2Vec 是现代 LLM embedding 设计的思想起点。

---

## 14. 面试高频问题 Checklist

- Skip-gram vs CBOW 区别？
- 为什么需要 input / output 两套向量？
- Negative Sampling 为什么有效？
- 为什么负采样用 3/4？
- Word2Vec 的局限性是什么？
- 与 BERT / GPT embedding 有什么联系？

---

## 15. 一句话高手总结

> Word2Vec 的本质不是网络结构，而是通过精心设计的预测任务，让模型在大规模数据中学到可泛化的语义表示。

# 人话版Word2Vec

# Word2Vec 人话版详细原理说明

> 目标：  
> 不背公式、不装学术，  
> **真正用大白话理解 Word2Vec 在干什么、为什么能学出“语义”。**

---

## 一句话先讲清楚 Word2Vec 在干嘛

**Word2Vec 就是在干一件事：**

👉 让电脑通过不断玩「猜词游戏」，  
👉 自己慢慢悟出：  
👉 哪些词意思差不多，哪些词关系密切。

---

## 一、为什么需要 Word2Vec？

### 电脑原来怎么看词？

在很早的时候，电脑是这样看词的：

猫 = 00010000
狗 = 01000000
苹果 = 10000000

yaml
Copy code

在电脑眼里：
- 猫和狗：没关系  
- 猫和苹果：也没关系  

👉 这和人类理解完全不一样。

---

### 人是怎么理解词的？

人理解一个词，靠的是：

> **这个词通常和谁一起出现**

- “猫”：可爱、宠物、喂、抓老鼠  
- “狗”：宠物、遛、忠诚  

👉 所以我们知道：  
**猫 和 狗 意思接近**

---

## 二、Word2Vec 的核心想法（灵魂）

Word2Vec 的信仰只有一句话：

> **如果两个词经常出现在相似的环境里，那它们意思就差不多。**

它不去理解“意思”是什么，  
它只关心一件事：

👉 **这个词的“邻居”是谁**

---

## 三、Word2Vec 怎么给自己出作业？

Word2Vec 不需要人工标注，  
它直接从文章里自己“出题”。

### 举个简单例子

一句话：

> 我 喜欢 深度 学习

---

### Step 1：选一个中心词

比如选：
深度

yaml
Copy code

---

### Step 2：看它左右附近的词（窗口）

我 喜欢 深度 学习
↑

yaml
Copy code

---

### Step 3：生成训练样本

Word2Vec 会默默记下：

深度 → 喜欢
深度 → 学习

yaml
Copy code

👉 这就是训练数据  
👉 完全不需要人工标签

---

## 四、Word2Vec 在玩什么“游戏”？

最常见的玩法叫 **Skip-gram**。

### 游戏规则很简单：

> **给你一个词，让你猜：  
> 它旁边可能会出现什么词？**

例如：

给你：深度
你应该猜：学习、喜欢

yaml
Copy code

---

## 五、词在电脑里到底长什么样？

在 Word2Vec 里：

👉 **每个词都会被变成一串数字（向量）**

而且**同一个词有两套向量**：

| 向量类型 | 人话解释 |
|---|---|
| 输入向量 | 当这个词拿来“出题” |
| 输出向量 | 当这个词拿来“当答案” |

比如：
- “深度”当题目 → 用输入向量  
- “学习”当答案 → 用输出向量  

---

## 六、它怎么判断“猜得好不好”？

Word2Vec 用了一个极其朴素的原则：

> **如果两个词很搭，就给高分；  
> 不搭，就给低分。**

这个“搭不搭”本质上就是：

👉 **两个向量靠不靠近**

---

## 七、现实问题：词太多，根本算不动

假设词典里有 100 万个词。

如果每次都问：

> “深度”后面是这 100 万个词里的哪一个？

👉 电脑直接崩溃。

---

## 八、Negative Sampling（超级聪明的偷懒）

Word2Vec 想了一个非常聪明的办法：

### 不再问：

> “这么多词里哪个是对的？”

### 而是改成问：

> **“这个词对不对？  
> 那几个词肯定不对吧？”**

---

### 举个非常直观的例子

中心词：
深度

shell
Copy code

#### 正确搭配：
学习 ✔

shell
Copy code

#### 随机找几个明显不搭的：
香蕉 ✖
冰箱 ✖
马路 ✖

yaml
Copy code

---

### 训练目标就变成了：

- “深度 + 学习” → 拉近  
- “深度 + 香蕉” → 推远  

👉 不用管全世界  
👉 只要分清「像不像一对」

---

## 九、训练时模型到底在干嘛？

你可以把训练想象成：

### 每一小步都在做三件事：

1. **把正确的词拉近**
深度 ←→ 学习

markdown
Copy code

2. **把不相关的词推远**
深度 ←—— 香蕉

yaml
Copy code

3. 重复几亿次

---

## 十、为什么最后就“懂语义”了？

因为：

- “猫” 被反复拉向：
- 宠物、可爱、喂
- “狗” 被反复拉向：
- 宠物、遛、忠诚

👉 最终：
- 猫 和 狗 离得很近  
- 猫 和 冰箱 离得很远  

**电脑就靠统计“悟”出了语义。**

---

## 十一、那种神奇的减法是怎么来的？

你可能见过：

国王 - 男人 + 女人 ≈ 王后

yaml
Copy code

用人话说就是：

- 国王 ≈ 男人 + 权力
- 王后 ≈ 女人 + 权力

Word2Vec 发现：
- 性别这个因素
- 在很多词里方向是一样的

👉 所以向量可以“算关系”。

---

## 十二、Word2Vec 的明显缺点

### ❌ 一个词只有一个意思

- bank（银行 / 河岸）  
👉 Word2Vec 分不清

### ❌ 看不懂整句话

它只看局部邻居。

---

## 十三、那它为什么这么重要？

因为它**第一次证明了一件事**：

> **只靠“预测上下文”，  
> 电脑真的能学会语言里的语义结构。**

后来的：
- BERT
- GPT
- 大语言模型

👉 都是在玩一个**更高级的猜词游戏**。

---

## 十四、一句话人话总结

> Word2Vec 就是让电脑反复玩“猜邻居”的游戏：  
> 猜对的词拉近，猜错的词推远。  
> 玩久了，电脑就发现：  
> 原来常一起出现的词，意思真的差不多。


